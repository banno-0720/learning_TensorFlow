{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzn51MYnC1dPmL+6I2qbC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/learning_TensorFlow/blob/main/YOLO_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwQteuZ_UdcW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import seaborn as sns### visualizations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "from datetime import datetime\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
        "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
        "                                     RandomContrast, Rescaling, Resizing, Reshape, LeakyReLU)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
        "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
        "from tensorflow.keras.regularizers import L2, L1\n",
        "from tensorflow.keras.initializers import RandomNormal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjIGOKcUikaF"
      },
      "source": [
        "# Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-7kfZVNUmlO",
        "outputId": "af095a83-ad2b-4b26-cc19-13fe7388f86a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "K2CFCBRFU66m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/huanghanchina/pascal-voc-2012\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClH2P8jbU-08",
        "outputId": "8d849fdf-4329-4b59-8f65-0273ba74ed5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: himanshugoyal2004\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/huanghanchina/pascal-voc-2012\n",
            "Downloading pascal-voc-2012.zip to ./pascal-voc-2012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.63G/3.63G [00:41<00:00, 94.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_list=['2007_000027.jpg','2007_000032.jpg','2007_000033.jpg','2007_000039.jpg','2007_000042.jpg','2007_000061.jpg',\n",
        "          '2007_000063.jpg','2007_000068.jpg','2007_000121.jpg','2007_000123.jpg','2007_000129.jpg','2007_000170.jpg',\n",
        "          '2007_000175.jpg','2007_000187.jpg','2007_000241.jpg','2007_000243.jpg','2007_000250.jpg','2007_000256.jpg',\n",
        "          '2007_000272.jpg','2007_000323.jpg','2007_000332.jpg','2007_000333.jpg','2007_000346.jpg','2007_000363.jpg',\n",
        "          '2007_000364.jpg','2007_000392.jpg','2007_000423.jpg','2007_000452.jpg','2007_000464.jpg','2007_000480.jpg',\n",
        "          '2007_000491.jpg','2007_000504.jpg','2007_000515.jpg','2007_000528.jpg','2007_000529.jpg','2007_000549.jpg',\n",
        "          '2007_000559.jpg','2007_000572.jpg','2007_000584.jpg','2007_000629.jpg','2007_000636.jpg','2007_000645.jpg',\n",
        "          '2007_000648.jpg','2007_000661.jpg','2007_000663.jpg','2007_000664.jpg','2007_000676.jpg','2007_000713.jpg',\n",
        "          '2007_000720.jpg','2007_000727.jpg','2007_000733.jpg','2007_000738.jpg','2007_000762.jpg','2007_000768.jpg',\n",
        "          '2007_000783.jpg','2007_000793.jpg','2007_000799.jpg','2007_000804.jpg','2007_000807.jpg','2007_000822.jpg',\n",
        "          '2007_001299.jpg','2007_001311.jpg','2007_001321.jpg','2007_001340.jpg']"
      ],
      "metadata": {
        "id": "mOrLWS0cVGum"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images='/content/pascal-voc-2012/VOC2012/JPEGImages/'\n",
        "train_maps='/content/pascal-voc-2012/VOC2012/Annotations/'\n",
        "\n",
        "val_images='/content/pascal-voc-2012/VOC2012/ValJPEGImages/'\n",
        "val_maps='/content/pascal-voc-2012/VOC2012/ValAnnotations/'\n",
        "\n",
        "classes=['aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable',\n",
        "         'dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
        "\n",
        "B=2\n",
        "N_CLASSES=len(classes)\n",
        "H,W =224,224\n",
        "SPLIT_SIZE=H//32\n",
        "print(SPLIT_SIZE)\n",
        "N_EPOCHS=135\n",
        "BATCH_SIZE=32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyIX70dMVXwp",
        "outputId": "32b07f7f-a86d-47d5-db56-87e773cee5c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/pascal-voc-2012/VOC2012/ValJPEGImages/\n",
        "!mkdir /content/pascal-voc-2012/VOC2012/ValAnnotations/"
      ],
      "metadata": {
        "id": "WDb7jRNMVbe3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in val_list:\n",
        "  shutil.move(train_maps+name[:-3]+\"xml\", val_maps+name[:-3]+\"xml\")"
      ],
      "metadata": {
        "id": "eE-7T_u9V9-R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in val_list:\n",
        "  shutil.move(train_images+name, val_images+name)"
      ],
      "metadata": {
        "id": "xpl7JstuWDZD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZSKZSsxioCm"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Fz3rHnO2g8b7"
      },
      "outputs": [],
      "source": [
        "def preprocess_xml(filename):\n",
        "  tree = ET.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  size_tree = root.find('size')\n",
        "  height = float(size_tree.find('height').text)\n",
        "  width = float(size_tree.find('width').text)\n",
        "  bounding_boxes=[]\n",
        "  for object_tree in root.findall('object'):\n",
        "    for bounding_box in object_tree.iter('bndbox'):\n",
        "      xmin = (float(bounding_box.find('xmin').text))\n",
        "      ymin = (float(bounding_box.find('ymin').text))\n",
        "      xmax = (float(bounding_box.find('xmax').text))\n",
        "      ymax = (float(bounding_box.find('ymax').text))\n",
        "      break\n",
        "    class_name = object_tree.find('name').text\n",
        "    class_dict={classes[i]:i for i in range(len(classes))}\n",
        "    bounding_box = [\n",
        "        (xmin+xmax)/(2*width),(ymin+ymax)/(2*height),(xmax-xmin)/width,\n",
        "        (ymax-ymin)/height,class_dict[class_name]]\n",
        "    bounding_boxes.append(bounding_box)\n",
        "  return tf.convert_to_tensor(bounding_boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XLv4PTY2g_2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb8e9d9-9eba-4de6-8df2-ee48070e3997"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
              "array([[ 0.479     ,  0.4644128 ,  0.542     ,  0.37366548,  0.        ],\n",
              "       [ 0.33      ,  0.37544483,  0.128     ,  0.12455516,  0.        ],\n",
              "       [ 0.408     ,  0.727758  ,  0.036     ,  0.17437722, 14.        ],\n",
              "       [ 0.07      ,  0.7597865 ,  0.036     ,  0.17437722, 14.        ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "preprocess_xml(val_maps+\"2007_000032.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sTFXxcoeg__c"
      },
      "outputs": [],
      "source": [
        "def generate_output(bounding_boxes):\n",
        "  output_label=np.zeros((SPLIT_SIZE,SPLIT_SIZE,N_CLASSES+5))\n",
        "  for b in range(len(bounding_boxes)):\n",
        "    grid_x=bounding_boxes[...,b,0]*SPLIT_SIZE\n",
        "    grid_y=bounding_boxes[...,b,1]*SPLIT_SIZE\n",
        "    i=int(grid_x)\n",
        "    j=int(grid_y)\n",
        "\n",
        "    output_label[i,j,0:5]=[1.,grid_x%1,grid_y%1,bounding_boxes[...,b,2],bounding_boxes[...,b,3]]\n",
        "    output_label[i,j,5+int(bounding_boxes[...,b,4])]=1.\n",
        "\n",
        "  return tf.convert_to_tensor(output_label,tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Muqtk2qwhABY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbdf194-178e-4208-eee3-7e4db8b3db30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(25,), dtype=float32, numpy=\n",
              "array([1.        , 0.35299993, 0.25088978, 0.542     , 0.37366548,\n",
              "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "generate_output(preprocess_xml(val_maps+\"2007_000032.xml\"),)[3][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DQjSB7Zn7FlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeaa6d18-e78a-41dc-bc1d-1ace44e00c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17061 17061\n",
            "64 64\n"
          ]
        }
      ],
      "source": [
        "im_paths=[]\n",
        "xml_paths=[]\n",
        "\n",
        "val_im_paths=[]\n",
        "val_xml_paths=[]\n",
        "\n",
        "for i in os.listdir(train_maps):\n",
        "\n",
        "  im_paths.append(train_images+i[:-3]+'jpg')\n",
        "  xml_paths.append(train_maps+i)\n",
        "\n",
        "for i in os.listdir(val_maps):\n",
        "\n",
        "  val_im_paths.append(val_images+i[:-3]+'jpg')\n",
        "  val_xml_paths.append(val_maps+i)\n",
        "\n",
        "print(len(im_paths),len(xml_paths))\n",
        "print(len(val_im_paths),len(val_xml_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-sl3BwUjAwpU"
      },
      "outputs": [],
      "source": [
        "train_dataset=tf.data.Dataset.from_tensor_slices((im_paths,xml_paths))\n",
        "val_dataset=tf.data.Dataset.from_tensor_slices((val_im_paths,val_xml_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5CdChkN4AwsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b50629-9606-49b4-f6bc-a3017a7dacd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'/content/pascal-voc-2012/VOC2012/ValJPEGImages/2007_001340.jpg'>, <tf.Tensor: shape=(), dtype=string, numpy=b'/content/pascal-voc-2012/VOC2012/ValAnnotations/2007_001340.xml'>)\n"
          ]
        }
      ],
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Lrg46DOlOZTH"
      },
      "outputs": [],
      "source": [
        "def get_imbboxes(im_path,xml_path):\n",
        "  img=tf.io.decode_jpeg(tf.io.read_file(im_path))\n",
        "  img=tf.cast(tf.image.resize(img, [H,W]),dtype=tf.float32)\n",
        "\n",
        "  bboxes=tf.numpy_function(func=preprocess_xml, inp=[xml_path], Tout=tf.float32)\n",
        "  return img,bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EOLIwSITBxmJ"
      },
      "outputs": [],
      "source": [
        "train_dataset=train_dataset.map(get_imbboxes)\n",
        "val_dataset=val_dataset.map(get_imbboxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6dKTo0JYBxom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b47aa47-056f-4e86-f506-ba078b079a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) (6, 5)\n"
          ]
        }
      ],
      "source": [
        "for i,j in train_dataset.skip(2):\n",
        "  print(i.shape,j.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "35mMR4qphF7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a228c86-46f5-4537-c585-a33c83495575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) (1, 5)\n"
          ]
        }
      ],
      "source": [
        "for i,j in val_dataset.skip(2):\n",
        "  print(i.shape,j.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8Ec-he4DBxq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bd19c4-4bad-4065-fc45-0e798ac5abdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "cv2.imwrite('out_1.jpg',np.array(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SMVz42ht7XUv"
      },
      "outputs": [],
      "source": [
        "transforms = A.Compose([\n",
        "    A.Resize(H,W),\n",
        "    A.RandomCrop(\n",
        "         width=np.random.randint(int(0.9*W),W),\n",
        "         height=np.random.randint(int(0.9*H),H), p=0.5),\n",
        "    A.RandomScale(scale_limit=0.1, interpolation=cv2.INTER_LANCZOS4,p=0.5),\n",
        "    A.HorizontalFlip(p=0.5,),\n",
        "    A.Resize(H,W),\n",
        "\n",
        "], bbox_params=A.BboxParams(format='yolo', ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uVHsFUUNeZGV"
      },
      "outputs": [],
      "source": [
        "def aug_albument(image,bboxes):\n",
        "  augmented=transforms(image=image,bboxes=bboxes)\n",
        "  return [tf.convert_to_tensor(augmented[\"image\"],dtype=tf.float32),\n",
        "          tf.convert_to_tensor(augmented[\"bboxes\"],dtype=tf.float32)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "R0U_N4rMS1C-"
      },
      "outputs": [],
      "source": [
        "def process_data(image,bboxes):\n",
        "    aug= tf.numpy_function(func=aug_albument, inp=[image,bboxes], Tout=(tf.float32,tf.float32))\n",
        "    return aug[0],aug[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZNjL4DAzgpGo"
      },
      "outputs": [],
      "source": [
        "train_dataset=train_dataset.map(process_data)\n",
        "val_dataset=val_dataset.map(process_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwVETxojgpJh",
        "outputId": "ee2a84af-eea8-407c-9653-4c7df9d227ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) tf.Tensor([[ 0.478      0.5720721  0.176      0.4354354 11.       ]], shape=(1, 5), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "for i,j in val_dataset.skip(2):\n",
        "  print(i.shape,j)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX4ojxfQgpLy",
        "outputId": "42984463-a849-42e7-8694-fb68e0b4a1ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "cv2.imwrite('out_2.jpg',np.array(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brb-LvyVcTBD",
        "outputId": "edcdaa91-d3a0-40ef-cead-0ec48dc22dd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD2lgwnicTGb",
        "outputId": "bf6f31a7-a877-4eeb-9879-890eb2f3465c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx9PGrLxd4x1",
        "outputId": "9549e33e-23dc-4b4b-baa7-bffa871f5b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) (1, 5)\n"
          ]
        }
      ],
      "source": [
        "for i,j in train_dataset.take(1):\n",
        "  print(i.shape,j.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqCg-aKhd8J_",
        "outputId": "33a66721-a89a-40d1-de83-47b6799dfedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) (4, 5)\n"
          ]
        }
      ],
      "source": [
        "for i,j in val_dataset.take(1):\n",
        "  print(i.shape,j.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aUsA7y7KxwNi"
      },
      "outputs": [],
      "source": [
        "def preprocess_augment(img,y):\n",
        "  img = tf.image.random_brightness(img, max_delta=50.)\n",
        "  img = tf.image.random_saturation(img, lower=0.5, upper=1.5)\n",
        "  img = tf.image.random_contrast(img, lower=0.5, upper=1.5)\n",
        "  #img = tf.image.random_hue(img, max_delta=0.5 )\n",
        "  img = tf.clip_by_value(img, 0, 255)\n",
        "  labels=tf.numpy_function(func=generate_output, inp=[y], Tout=(tf.float32))\n",
        "  return img,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "D8ZsLcjfgih2"
      },
      "outputs": [],
      "source": [
        "def preprocess(img,y):\n",
        "  #img = tf.cast(tf.image.resize(img, size=[H, W]), dtype=tf.float32)\n",
        "  img = tf.cast(img, dtype=tf.float32)\n",
        "  labels=tf.numpy_function(func=generate_output, inp=[y], Tout=(tf.float32))\n",
        "  return img,labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=train_dataset.map(preprocess_augment)"
      ],
      "metadata": {
        "id": "BXTpvVMHOyJg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "C3ViaB8lgikP"
      },
      "outputs": [],
      "source": [
        "val_dataset=val_dataset.map(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZVMqsnNYOsD",
        "outputId": "1705ac48-1288-4b82-d94d-0081f43dcd37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w54dj2Fgirq",
        "outputId": "d47ed070-d5da-45cc-db46-bf9ae1472871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3) (7, 7, 25)\n"
          ]
        }
      ],
      "source": [
        "for i,j in train_dataset.take(1):\n",
        "  print(i.shape,j.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7Tol9jkULK5",
        "outputId": "a94e37c8-d77c-41cf-8e1a-56cc4fcaee1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "cv2.imwrite('out_3.jpg',np.array(i[2]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-OeMOfEO8fH",
        "outputId": "a5928e4f-a265-471d-e027-f90a8dcbf8e2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EVLla-p-Yf4D"
      },
      "outputs": [],
      "source": [
        "def ensure_shape(image, label):\n",
        "    \"\"\"\n",
        "    Ensures the shape of the image and label tensors is defined.\n",
        "    \"\"\"\n",
        "    image.set_shape([H, W, 3]) # or your expected input shape\n",
        "    label.set_shape([7, 7, 25]) # or your expected label shape\n",
        "    return image, label\n",
        "\n",
        "train_dataset = train_dataset.map(ensure_shape)\n",
        "val_dataset = val_dataset.map(ensure_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "1GptrY4-gimh"
      },
      "outputs": [],
      "source": [
        "train_dataset=(\n",
        "  train_dataset.\n",
        "  batch(BATCH_SIZE).\n",
        "  prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7qEQp4RPgipK"
      },
      "outputs": [],
      "source": [
        "val_dataset=(\n",
        "  val_dataset.\n",
        "  batch(BATCH_SIZE).\n",
        "  prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ppVkF8BYf6u",
        "outputId": "fcfd55a8-cd2a-477d-f168-4034685c6154"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7, 7, 25), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJsw7WPTY63m",
        "outputId": "7b33de04-c1b4-4c0d-b95e-d4d002bc2486"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7, 7, 25), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RQs21PkWFoj"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}